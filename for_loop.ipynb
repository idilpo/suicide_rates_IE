{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_urls = []\n",
    "\n",
    "start_date = \"2006-07-02\"\n",
    "end_date = \"2006-07-12\"\n",
    "base_url = \"https://rip.ie/death-notice/s/all?page={}&start={}%2B00%3A00%3A00&end={}&sortField=a.createdAtCastToDate&sortDir=DESC&view=boxes\"\n",
    "page_count = 10 #manually changed every time\n",
    "#rough_count_deaths = page_count*40\n",
    "#print(str(rough_count_deaths) + \" deaths between \" + start_date + \" and \" + end_date)\n",
    "\n",
    "for i in range(1, page_count+1):\n",
    "    entry_urls.append(base_url.format(i, start_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing {url}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_html(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(soup):\n",
    "    url_list = []\n",
    "    anchors = soup.find_all('a', class_='showdown-dn-link')\n",
    "    urls = [anchor['href'] for anchor in anchors]\n",
    "    for url in urls:\n",
    "        url_list.append(\"https://rip.ie\" + url)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_date(soup):\n",
    "    date_list = {}\n",
    "\n",
    "    published_date_value = soup.find('p', class_='DeathNotice_dates-published-date__M0A_i')\n",
    "    if published_date_value:\n",
    "        date_list[\"Published Date\"] = published_date_value.get_text()\n",
    "    else:\n",
    "        date_list[\"Published Date\"] = None\n",
    "\n",
    "    death_date_value = soup.find('p', class_='DeathNotice_dates-death-date__bR7g_')\n",
    "    if death_date_value:\n",
    "        date_list[\"Death Date\"] = death_date_value.get_text()\n",
    "    else:\n",
    "        date_list[\"Death Date\"] = None\n",
    "\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obituary(soup):\n",
    "    description = soup.find('div', class_='DeathNotice_description__sY_tC word-break')\n",
    "    if description:\n",
    "        text = description.get_text()\n",
    "        text = text.strip()\n",
    "        text = text.replace(\"Â\", \"\")\n",
    "        text = text.replace(\"nee\", \"née\")\n",
    "        text = text.replace(\"nÃ©e\", \"née\")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = text.replace(\"â\", \"’\")\n",
    "        return text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc(url_list):\n",
    "    url_info = {}\n",
    "    url_pattern = r'https://rip.ie/death-notice/([^/]+)-([^/]+)-([^/]+)(?:-([^/]+))?-(\\d+)'\n",
    "    \n",
    "    for u in url_list:\n",
    "        m = re.match(url_pattern, u)\n",
    "            \n",
    "        if m:\n",
    "            url_id = m.group(5)\n",
    "            city = m.group(2)\n",
    "            loc = m.group(3)\n",
    "\n",
    "            url_info[url_id] = {\n",
    "                \"City\": city,\n",
    "                \"Location\": loc\n",
    "            }\n",
    "    return url_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_data(page_data):\n",
    "    data_list = []\n",
    "    if page_data:\n",
    "        url_list = get_url(scrape_html(page_data))\n",
    "            \n",
    "        for u in url_list:\n",
    "            s = scrape_html(re_url(u))\n",
    "            url_id = url.split(\"-\")[-1]\n",
    "            name = get_name(s)\n",
    "            dates = get_individual_date(s)\n",
    "            obituary_text = get_obituary(s)\n",
    "\n",
    "            data_list.append({\n",
    "                                \"Name\": name,\n",
    "                                \"ID\": url_id,\n",
    "                                \"City\": get_loc(url_list[url_id[\"City\"]]),\n",
    "                                \"Location\": get_loc(url_list[url_id[\"Location\"]]),\n",
    "                                \"Published Date\": dates[\"Published Date\"],\n",
    "                                \"Death Date\": dates[\"Death Date\"],\n",
    "                                \"Obituary Text\": obituary_text\n",
    "                            })\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1297f9afe972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrip_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_page_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrip_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-434d23945eef>\u001b[0m in \u001b[0;36mget_page_data\u001b[0;34m(page_data)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpage_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0murl_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscrape_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2218bfcb71cd>\u001b[0m in \u001b[0;36mscrape_html\u001b[0;34m(html)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mrejections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         for (self.markup, self.original_encoding, self.declared_html_encoding,\n\u001b[0m\u001b[1;32m    343\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_replacement_characters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m              self.builder.prepare_markup(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mprepare_markup\u001b[0;34m(self, markup, user_specified_encoding, document_declared_encoding, exclude_encodings)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;31m# Ask UnicodeDammit to sniff the most likely encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mtry_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser_specified_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_declared_encoding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n\u001b[0m\u001b[1;32m    364\u001b[0m                                exclude_encodings=exclude_encodings)\n\u001b[1;32m    365\u001b[0m         yield (dammit.markup, dammit.original_encoding,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/bs4/dammit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, override_encodings, smart_quotes_to, is_html, exclude_encodings)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/bs4/dammit.py\u001b[0m in \u001b[0;36mencodings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# declaration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeclared_encoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             self.declared_encoding = self.find_declared_encoding(\n\u001b[0m\u001b[1;32m    302\u001b[0m                 self.markup, self.is_html)\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_usable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeclared_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtried\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/bs4/dammit.py\u001b[0m in \u001b[0;36mfind_declared_encoding\u001b[0;34m(cls, markup, is_html, search_entire_document)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mhtml_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'html'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mdeclared_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mdeclared_encoding_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxml_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxml_endpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdeclared_encoding_match\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_html\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mdeclared_encoding_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhtml_endpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "rip_df = get_page_data(entry_urls)\n",
    "rip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '/Users/idilbilgic/Desktop/STAGE4.1/COMP30170/suicide_rates_IE/output_monthly'\n",
    "rip_df.to_csv(os.path.join(output_directory, 'rip_output_2006_07.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rip_df.to_csv('rip_output_2006.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
